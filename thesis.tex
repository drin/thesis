% Cal Poly Thesis
% based on UC Thesis format
% modified by Mark Barry 2/07.
\documentclass[12pt]{ucthesis}

% Different font in captions (single-spaced, bold) ------------
\newcommand{\captionfonts}{\small\bf\ssp}

\makeatletter  % Allow the use of @ in command names
\long\def\@makecaption#1#2{%
  \vskip\abovecaptionskip
  \sbox\@tempboxa{{\captionfonts #1: #2}}%
  \ifdim \wd\@tempboxa >\hsize
    {\captionfonts #1: #2\par}
  \else
    \hbox to\hsize{\hfil\box\@tempboxa\hfil}%
  \fi
  \vskip\belowcaptionskip}
\makeatother   % Cancel the effect of \makeatletter
% ---------------------------------------

%formatting/structure based packages
   \usepackage[letterpaper]{geometry}
   \usepackage[overload]{textcase}

%packages necessary for content
   \usepackage{url}
   \usepackage{graphicx}
   \usepackage{amssymb}
   \usepackage{amsmath}
   \usepackage{algorithmicx}
   \usepackage{algpseudocode}
   \usepackage{subfig}
   \usepackage{multirow}

   \usepackage[utf8]{inputenc}
   \usepackage[russian,english]{babel}

%paper formatting
   \setlength{\parindent}{0.25in} \setlength{\parskip}{6pt}
   \geometry{verbose,nohead,tmargin=1.25in,bmargin=1in,lmargin=1.5in,rmargin=1.3in}
   \setcounter{tocdepth}{2}

\begin{document}

% Declarations for Front Matter
\title{Aldrin's Thesis}
\author{Aldrin Montana}
\degreemonth{June}
\degreeyear{2013}
\degree{Master of Science}
\field{Computer Science}
\campus{San Luis Obispo}

\defensemonth{June}
\defenseyear{2013}

\numberofmembers{5}
   \chair{Alexander Dekhtyar, Ph.D.}
   \othermemberA{Chris Lupo, Ph.D.}
   \othermemberB{Tim Kearns, Ph.D.}
   \othermemberC{Michael Black, Ph.D.}
   \othermemberD{Eriq Augustine, M.S.}
\copyrightyears{seven}

\maketitle
\begin{frontmatter}
   % Custom made for Cal Poly (by Mark Barry, modified by Andrew Tsui).
   \copyrightpage

   % Custom made for Cal Poly (by Andrew Tsui).
   \committeemembershippage

   \begin{abstract}
      Pyroprinting is a novel, library-dependent microbial source tracking
      (MST) method developed by the biology department at Cal Poly, San Luis
      Obispo. This method consists of two parts: (1) sample collection and
      pyroprinting (2) comparison of produced pyroprints against a database.
      Cal Poly Library of Pyroprints (CPLOP) is a web-based database
      application that provides storage and analysis of pyroprints. CPLOP
      currently contains 4500 pyroprints and is growing rapidly as students and
      researchers continue adding data. Agglomerative hierarchical clustering,
      the traditional method of analysis used by biologists, is inadequate.
      While the clusters it produces are acceptable, pyroprinting requires a
      new method of analysis that is both scalable and flexible. Agglomerative
      hierarchical clustering falls short in these regards as it is unable to
      efficiently cluster new data with already clustered data and it is
      incapable of utilizing available metadata. We propose ontology-based
      hierarchical clustering (\textit{OHClust!}), a modification of
      hierarchical clustering that uses an ontology to direct the order in
      which data is clustered. In this paper we discuss the strengths and
      weaknesses of OHClust! and analyze its performance in comparison to
      agglomerative hierarchical clustering.
   \end{abstract}

   \begin{acknowledgements}
      This work was supported in part by undergraduate education grants from:
      \begin{itemize}
         \item The W.M. Keck Foundation
         \item The National Science Foundation
      \end{itemize}
      I would like to thank:
      \begin{itemize}
         \item The department staff, especially Christy, my CSC mom.
         \item Emily Neal for being awesome and patient in working with me.
         \item My family and friends, with special thanks to my mom, step dad,
               and dad, for all that they've done to support me.
         \item Alex Dekhtyar for being the best adviser I could have asked for.
               I believe all of my growth can be attributed to the guidance
               that Alex has provided me.
         \item \foreignlanguage{russian}{квас} for always being there to soothe the soul.
         \item My roommates who have always provided me with a family away from
               home. Our unity is strong.
      \end{itemize}
   \end{acknowledgements}

   \tableofcontents

   \listoftables
   \listoffigures
\end{frontmatter}

\pagestyle{plain}

\renewcommand{\baselinestretch}{1.66}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%% ----------------- Main chapters ---------------------- %%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Introduction}\label{chap:intro}
   To maintain the cleanliness of current ``fishable and swimmable'' bodies of
   water, and improve the cleanliness of other bodies of water, health and
   environmental protection agencies have been interested in the ability to
   track sources of fecal contamination at the species
   level~\cite{Scott:CurrentMST, Simpson:StateOf, Desmarais:SoilInfluence}.
   Identifying the animal species from which a fecal contamination originates
   is an important first step in controlling fecal contamination and managing
   bacterial risks by enabling natural resource managers to address the problem
   of fecal contamination at its source. Investigation of fecal contamination
   is especially necessary in waters used for human recreation where dangerous
   pathogens are transferable. 

   Within the last decade, several techniques of identifying and descriminating
   between various sources of fecal bacteria in an environment have been
   studied and developed~\cite{Sargeant:ReviewMST, Hagedorn:MST_TMDL,
   Lowe:FocusMST, Rivera:MSTCharacterization, Cornelison:MSTTools,
   Chase:FloridaMST}. This field of study, \textit{Microbial source tracking}
   (MST) dates back to the 1960s with the use of fecal coliform-to-fecal
   streptococci ratios. Many MST techniques rely on the genotypic or
   phenotypic analyses of fecal indicator bacteria (FIB) such as fecal
   coliforms, fecal enterococci, or \textit{Escherichia coli} (\textit{E.
   coli}). FIB are bacteria that are found in the intestinal tracts of various
   host animals, and are still present in the host animal's stool. When found,
   FIB can be used to indicate or approximate the level of fecal contamination
   in, or near, a body of water~\cite{Simpson:StateOf}. In 1999, the first
   review of MST techniques was published by Debby
   Sargeant~\cite{Sargeant:Methods}, followed by another review in November
   2011~\cite{Sargeant:ReviewMST}. In her 2011 review of current MST methods,
   she describes the MST field as a new, experimental field that has been
   continuously studying and developing new methods over several
   years~\cite{Sargeant:ReviewMST}. Fortunately, there have been publications
   that provide notable overviews of MST techniques and
   applications~\cite{Hagedorn:CaseStudies, Domingo:Current,
   Harwood:RapidMethods} within the last couple of years. Despite the highly
   experimental nature of MST at this phase of its development, Sargeant claims
   that there is strong pressure on natural resource managers to use these
   techniques to identify bacterial sources of
   pollution~\cite{Sargeant:ReviewMST}.

   This thesis addresses the comparison of DNA fingerprints of
   bacterial isolates for a library-dependent MST technique developed by the
   Cal Poly Biology department. As described by Sargeant, a library-dependent
   MST technique relies on a database, called the library, which contains
   genotypic information of the FIB being studied. For the work presented in
   this thesis, the library consists of DNA fingerprints from \textit{E. coli}
   isolates. The workflow for a library-dependent MST technique starts with the
   collection of a large number (varies based on the size and diversity of the
   environment being studied) of bacterial isolates from a variety of known
   host species. Specifically, isolates of a particular FIB are collected, based on
   the type of environment and animals in the study. Then, genotypic analysis is
   then applied to collected bacterial isolates and resulting data is stored in
   the library. The accumulated genotype information of bacterial isolates
   composes the required characterization of \textit{bacterial strains} in the
   target environment necessary for determining the host animal species from
   which collected bacterial isolates originate. The definition of a strain
   varies from research group to research group, but in this work a bacterial
   strain is a group of bacterial isolates that have similar genotypes. Once
   the library has reached an appropriate size for the environment of interest,
   it is possible to collect bacterial isolates with unknown host specie
   origins from the environment and compare its genotype to bacterial isolates in
   the library. The comparison can then identify the bacterial strain most
   similar to the collected bacterial isolate and determine the host specie
   from which it originates. More specifically, for the work described in this
   thesis, bacterial isolates are considered part of the same strain if their
   DNA fingerprints are determined to be similar. Although not conclusively
   established, this type of MST technique relies on the relationship between
   bacterial strains and various host species. For instance, there are many
   different kinds of \textit{E. coli}, some of which are only present in
   humans, while others are only found in other species or particular regions
   of the world. With this consideration, the library must be populated with
   bacterial isolates collected from known species so that relationships
   between bacterial strains and host species can emerge. Additionally, the
   selection of an appropriate FIB is important and will affect interpretations
   of analyses differently. Unfortunately, since the definition of a bacterial
   strain is fuzzy and dependent on the research group and research task at
   hand, the level of similarity required for two organisms be considered part
   of the same strain is defined and established differently from research
   group to research group.

   Interestingly, Sargeant claims that the MST field is moving away from
   library-dependent methods and towards library-independent methods. While
   the discussion of this topic is outside the scope of this thesis, it's
   worthwhile to mention that the reason for this is that library-dependent
   methods require a significant effort to build an appropriately large, useful
   library. The library also needs to include fingerprints relative to the
   environment being investigated. However, it is currently a more reliable,
   flexible MST technique that allows for the identification of hosts beyond
   humans and some domestic animal species.

   Generally, library-dependent methods being developed have shifted
   significantly towards genotypic analysis of collected bacteria and away from
   phenotypic analysis. Genotypic analysis strives to separate observed
   bacterial samples by differentiating between bacteral strains based on
   fingerprints of the bacterial genome. When compared to phenotypic methods,
   genotypic methods are advantageous since they use differences in nucleic
   acids to distinguish between strains of \textit{E. coli} with greater
   sensitivity~\cite{Anderson:Diversity, Gordon:StrainTyping, Ochman:Enzyme,
   Scott:CurrentMST, Simpson:StateOf}.
   
   The contributions of this thesis consist of three primary artifacts:
   \begin{itemize}
      \item A tool for analyzing the pyrosequencing process in-silico.
      \item An interim tool for analyzing pyroprints before the creation of
            CPLOP~\cite{Jan:Thesis}.
      \item A clustering algorithm, OHClust!, designed to be incremental and
            flexible in its analysis.
   \end{itemize}
   The work described in this thesis has been used by M.S. and B.S. students in
   Biology for various types of bacterial research, including longitudinal
   population-based studies and studies of bacterial transferrance between host
   species. These studies used \textit{pyroprinting}, a library-dependent MST
   method developed by the Biology department at Cal Poly, San Luis Obispo,
   further described in Section \ref{sec:pyroprinting}. The biologists
   initially needed a way to select a dispensation sequence to maximize the
   effectiveness of pyroprinting for MST analysis. During this time a tool was
   developed that would characterize permutations of cycles of nucleotides on
   given \textit{E. Coli} rRNA sequences, further detailed in Chapter
   \ref{chap:other_contributions}. Once a dispensation sequence had been
   determined, the biologists' needs shifted towards a tool that could provide
   analysis of data in spreadsheets until a database-based application could be
   developed, a void now filled by CPLOP--a database-based web application for
   the storage and retrieval of pyroprints~\cite{Jan:Thesis}. This analysis
   tool is described in \ref{chap:other_contributions}. Finally, chapters
   \ref{chap:algorithm}, \ref{chap:related}, and \ref{chap:evaluation}, focuses
   on the development and characterization, related work, and evaluation of the
   clustering algorithm, OHClust!, respectively. OHClust! is the algorithm
   for the computational analysis which determines what bacterial isolates are
   of the same strain.

\chapter{Background}\label{chap:background}
   To address some of the common complaints regarding other methods of strain
   differentiation, the Biology department at Cal Poly, San Luis Obispo has
   been developing a genotypic, library-dependent MST method,
   \textit{pyroprinting}. Current genotypic methods can be labor-intensive or
   financially expensive, and many have issues in reproducibility when
   identifying known \textit{E. coli} strains~\cite{Gordon:StrainTyping,
   Scott:CurrentMST, Simpson:StateOf}. This is especially evident in
   longitudinal single-host studies of microbial strains, which are a key
   problem for MST studies~\cite{Simpson:StateOf, Anderson:Diversity,
   Schlager:Clonal}. In these studies, bacterial cultures are typically grown
   from samples collected in roughly even intervals (daily, weekly, etc.) from
   a single individual. These studies are concerned with the total number of
   strains of a particular bacterium (e.g. \textit{E. coli}) present in a host
   at a given moment in time, strain turnover over time, the presence of
   dominant strains, change of strain dominance over time, as well as
   corresponding cause-and-effect questions (i.e. what causes all these
   changes)~\cite{Anderson:Diversity, Caugant:Diverse, Sears:Persist,
   Simpson:StateOf}.
   
   Several students in the Biology department have conducted research projects
   focusing on the use and applicability of pyroprinting. Among the various
   research projects, one student has established and piloted a novel framework
   for genotypic microbial analysis of bacteria (so far \textit{E. coli} only)
   in longitudinal studies~\cite{Montana:ChronoCluster, Montana:CRC}. This
   framework incorporates \textit{pyroprinting} and an \textit{in-silico}
   strain differentiation method based on OHClust!.

   This chapter fully explains the context of this thesis, and the research it
   supports, and establishes terminology that will be used throughout the rest
   of this thesis.

   \section{Bacterial Sampling and Data Collection}\label{sec:sampling}
      The process of sampling begins with the selection of stool, or fecal
      matter. From the stool, a number of samples are collected, isolated, and
      cultured in preparation for polymerase chain reaction (PCR) and
      pyroprinting. Each of the bacterial samples are then streaked onto
      MacConkey agar plates for single isolated colonies (isolate for short)
      and grown overnight. Isolates are confirmed for the target bacteria at
      the end of the process. Any isolates that are not of the target bacteria
      are left unused. This process is highlighted in a previous submission to
      the CSU research competition~\cite{Montana:CRC, CSUPERB} which describes
      a study where \textit{E. coli} was sampled once a month for six months
      from three individuals--two females and one male--ranging in age from
      $20-25$.

   \section{Pyroprinting}\label{sec:pyroprinting}
      Pyrosequencing is a DNA sequencing process where a new strand of DNA,
      which we will refer to as the \textit{sequencing strand}, is built
      incrementally, complementary to the strand of DNA being sequenced, which
      we will call the \textit{template strand}. Ronaghi, et al. describe the
      pyrosequencing process in more detail~\cite{ronaghi:shedsLight}. This
      process takes place on a \textit{plate} containing many
      \textit{wells}--$24$ wells for the \textbf{PyroMark Q24}, a
      pyrosequencing machine manufactured by Qiagen that is currently in use by
      the Biology department at Cal Poly, San Luis Obispo. A pyrosequencing
      \textit{run} denotes the entirety of the pyrosequencing process for a
      single plate. While only one plate can be processed per run, each well on
      the plate contains a separate pyrosequencing reaction. For the PyroMark
      Q24, this means that a single run can pyrosequence 24 separate template
      strands. During a run, nucleotides--\textbf{A}denine, \textbf{T}hymine,
      \textbf{C}ytosine, or \textbf{G}uanine--are dispensed into each well of
      the plate.
      
      The order in which nucleotides are dispensed is determined by
      a parameter called the \textit{dispensation sequence}. The dispensed
      nucleotide binds with the next available unbound nucleotide of the
      template strand if they are complementary--\textbf{A} binds with
      \textbf{T} and \textbf{C} binds with \textbf{G}. If the dispensed
      nucleotide binds to the template strand, the sequencing strand is
      extended and light is emitted. The observed light emittance is
      proportional to the amount of nucleotides incorporated into the
      sequencing strand. For example, if \textbf{A} is incorporated into the
      sequencing strand and the light emittance is measured at 100, then
      \textbf{T} is incorporated into the sequencing strand with light
      emittance 200, then the template strand has twice as many \textbf{T}
      nucleotides following a series of \textbf{A} nucleotides. Any remaining
      nucleotide not incorporated is enzymatically destroyed and the next
      nucleotide in the dispensation sequence is dispensed.
      
      The pyrosequencing machine measures and records the light emitted from
      the binding reaction to construct a \textit{pyrogram} of the template DNA
      strand. A pyrogram is a graph that plots light emittance of DNA synthesis
      reactions with the dispensed nucleotide, depicted in Figure
      \ref{fig:pyrogram}. Modern pyrosequencing equipment allows for
      dispensation sequences of up to 200 nucleotides in length, however, the
      quality of the sequencing process deteriorates beyond 100--120
      dispensations.

      Pyrosequencing is a relatively cheap and quick DNA sequencing method.
      That is to say, it is not as cheap and powerful as modern DNA sequencing
      processes known as ``Next Generation Sequencing'' (NGS), but it is one of
      the cheapest and quickest of non-NGS sequencing processes. While
      pyrosequencing is cheap compared to other academically accessible DNA
      sequencing processes, sequencing a strand of DNA in 100--200 nucleotide
      chunks for strain differentiation purposes becomes expensive very
      quickly. To adapt pyrosequencing for strain differentiation purposes, the
      Biology department decided to pyrosequence multiple strands of DNA in
      parallel in a well. This process, referred to as \textit{pyroprinting},
      leverages the fact that introduced nucleotides will bind with the first
      available nucleotide of the template strand by sequencing multiple
      template strands in the same reaction. This already takes place in the
      case where the template strands are identical (DNA amplification), but
      the template strands used in pyroprinting are not necessarily identical.

      Pyroprinting generates pyroprints (DNA fingerprints) based on the DNA
      from two highly variable regions of the microbial genome. These regions,
      called \textit{intergenic transcribed spacers} (ITS), are regions of the
      genome located between two genes that contains non-coding DNA. For
      microbial source tracking, the 16S--23S ITS region, located between the
      16S and 23S genes, is a widely accepted ITS region for comparing
      strains~\cite{Boyer:ITS, Roth:Phylo, Tyler:Primers}. Also mentioned in
      this paper is the 23S--5S ITS region. Both of these regions of DNA are
      non-coding, and thus are able to accumulate more variability, or
      mutations, than regions of DNA that code for important proteins or RNAs
      due to lack of selective pressures. This variability manifests in
      nucleotide changes in the genome, thus while the DNA of the 16S, 23S, and
      5S genes are conserved, the same is not necessarily true for the ITS
      region between them. However, these regions may be more or less useful
      depending on the choice of FIB.
      
      The use of the 16S--23S and 23S--5S ITS regions are especially important
      for the work described in this paper because the microbial genome has
      \textit{several copies} of each ITS region. The \textit{E. coli} genome,
      specifically, has seven replicates of both the 16S--23S and 23S--5S ITS
      regions~\cite{Boyer:ITS, Roth:Phylo, Tyler:Primers}. Pyroprinting relies
      on the use of each replicate of the target ITS region that exists in the
      microbial genome. That is, for a single microbial isolate, each copy of
      the 16S--23S ITS region is pyrosequenced together, and each copy of the
      23S--5S ITS region is pyrosequenced together. Thus, for a microbial
      isolate, two \textit{pyroprints} are constructed such that each pyroprint is
      an aggregate of several, possibly different, template strands. Operating
      under the assumption that the variability between ITS regions of the
      genome will vary for different strains of \textit{E. coli}, a pyroprint
      acts as a DNA fingerprint for each strain--a pyrogram fingerprint.
      
      \begin{figure}[t]
      \centering
      \includegraphics[width=\columnwidth]{graphics/pyroprinting.eps}
      \caption{Pyroprinting process: light intensities (black bars at the
               bottom) are observed for each nucleotide in the dispensation
               sequence. Open boxes represent conserved DNA sequences in the
               23S and 5S rRNA genes and \textnormal{S} indicates the point at
               which the sequencing primer binds to the DNA, beginning the
               sequencing process.}
      \label{fig:pyroprinting}
      \end{figure}

      \begin{figure}[t]
      \centering
      \includegraphics[width=\columnwidth]{graphics/pyrogram.eps}
      \caption{A pyrogram up to dispensation 22 (out of 104). The y-axis is
      light emittance, the x-axis is dispensation over time. When a nucleotide
      is dispensed, the light emitted peaks almost immediately when the
      reaction takes place then subsides as the reaction completes.}
      \label{fig:pyrogram}
      \end{figure}

   \section{Bacterial Strains}\label{sec:strains}
      Once bacterial isolates are collected and pyroprints of the isolates are
      constructed, bacterial strains are identified and determined. Bacterial
      strains are constructed in an isolate-major approach, that is, bacterial
      strains are constructed based on the analysis of bacterial isolates. While
      bacterial isolates are compared based on the similarity between their
      associated pyroprints, biological meaning can only be inferred from the
      relationships between bacterial isolates. Described further in Chapter
      \ref{chap:algorithm}, bacterial isolates are grouped into clusters using
      either agglomerative hierarchical clustering or OHClust!. The
      interpretation of resulting clusters is straightforward--each cluster
      corresponds to a potential bacterial strain. More specifically, a cluster
      is a group of similar data points determined by a similarity metric.
      Therefore, the similarity metric that compares isolates based on their
      pyroprints ensures that isolates that have similar genotypes, expressed
      in their pyroprints, are placed in the same clusters. In this way, each
      cluster that is formed by a clustering algorithm represents a group of
      isolates that have similar genotype, thus matching our definition of a
      bacterial strain.

   \section{Data and Terminology}\label{sec:data_and_terms}
      In this section, the details of the data, digital representation of said
      data, terminology, and relevant synonyms used for describing all of the
      above are explained. For consistency and simplicity, the terms that will
      be used throughout the paper are established here.

      \subsection{Data}
      A data point is an individual record or object of data to be compared
      and clustered with other individual records or objects of data. While
      this seems trivial, it is important to note that a data may be a scalar
      value, a vector of values, or any other complex representation.
      Throughout this paper, we will use the term \textbf{data point} to refer
      to individual data records or patterns.

      There are three primary biological components being analyzed--isolates,
      ITS regions, and pyroprints. For the analysis described in this paper,
      the primary data point of interest is the \textit{Isolate}. Generally,
      an isolate's genome has many genes on it, and there are particular
      ITS regions of interest--for this paper and the work done with the
      Biology department, only the 23S--5S and 16S--5S ITS regions are used.
      Computationally, we simply represent an isolate as an object
      which has a reference to a set of ITS regions. Since, in practice, a
      pyroprint is constructed by pyroprinting an ITS region of the isolate
      genome, we represent an ITS region as a collection of pyroprints. A
      Pyroprint is output from the pyrosequencer as a pyrogram of light
      emittance per nucleotide dispensation--a character in the alphabet
      \textbf{A}, \textbf{T}, \textbf{C}, or \textbf{G}. We easily represent a
      pyroprint as a vector of floating point values (light emissions) where
      each component corresponds to a dispensed nucleotide. In this way, the
      biological components and their relationships are represented in an
      object-oriented manner. More details are described in Section
      \ref{sec:design}. More formally, given a set of isolates $I$ and an
      isolate $i \in I$, a set of ITS regions $R$ such that an ITS region $r_i
      \in R$, and a dispensation sequence $D = (d_1, \ldots, d_n)$, where $d_n
      \in \{A, T, C, G\}$, a pyroprint $\bar{p_i} = (p_i1,\ldots p_in)$, where
      $p_in$ is a real number representing the light emitted during the
      pyrosequencing process of ITS region $i$ when the nucleotide $d_n$ was
      introduced on dispensation $n$.

      %\begin{itemize}
      Let $I$ be the set of all isolates. Let $R$ be the set of ITS regions
      associated with $i$ $\forall i \in I$ $R$. Let $A$ be the set of all upper
      thresholds, and $B$ be the set of all lower thresholds. $\forall r \in R$,
      $\forall \alpha \in A$, $\forall \beta \in B$ let $\alpha_r$, $\beta_r$ be the
      upper and lower threshold pair corresponding to the region $r$ such that
      $\alpha_r > \beta_r$. Let $P$ be the set of all pyroprints. $\forall i \in I$,
      $\forall r \in R$ let $P_{ir} \in P$ be the pyroprint corresponding to isolate
      $i$ at ITS region $r$. Let $M$ be the set of all similaritiy matrices.
      $\forall r \in R$ let $M_r \in M$ be the matrix containing all pairwise Pearson
      correlations such that $\forall i,j \in I \times I$ $M_r[i,j] = sim(P_{ir},
      P_{jr})$
      %\end{itemize}
      
      \subsection{Metrics}
      Comparison metrics are used to compare data points based on similarity
      or dissimilarity. While similarity and dissimilarity are two sides of
      the same coin, the ease of conversion between the two varies based on
      the comparison metric used. In this paper, our analyses are based on
      similarity values. This is so chosen because the comparison metric used
      for pyroprints is \textit{Pearson's Correlation}--a similarity metric
      defined in Equation \ref{eq:pearson}. Unfortunately, pyrosequencing machines
      are observe fluctuations in the amplitude of light emitted
      during binding reactions in the sequencing process. Pearson's correlation
      is ideal for accommodating these fluctuations because it is
      concerned with comparing the variance between a pair of pyroprints
      $\bar{p_x}$, $\bar{p_y}$ instead of a direct comparison between the
      values $p_xn$, $p_yn$.

      \begin{equation}\label{eq:pearson}
         sim(\bar{X},\bar{Y}) = \frac{\sum_{i=1}^{N}(x_i - E(\bar{X}))(y_i - E(\bar{Y}))}
         {\sqrt{\sum_{i=1}^{N}(x_i-E(\bar{X}))}\sqrt{\sum_{i=1}^{N}(y_i-E(\bar{Y}))}},
      \end{equation}
      \begin{equation}\label{eq:cluster_sim}
         sim(C_m, C_n) = i,j \in C_m \times C_n, \frac{\sum_{\forall r \in
         R}M_r[i,j]} {|C_m \times C_n|}
      \end{equation}
      \begin{equation}\label{eq:threshold}
         thr(sim(C_m, C_n)) = \begin{cases}
                              0 & if sim(C_m, C_n) < \beta_r \\
                              1 & if sim(C_m, C_n) > \alpha_r \\
                              sim(C_m, C_n) & otherwise \\
                              \end{cases}
      \end{equation}

      In order to appropriately interpret the computed correlation coefficient
      between a pair of pyroprints, we utilize a pair of threshold values
      $\alpha$, the \textit{upper threshold}, and $\beta$, the \textit{lower
      threshold}\footnote{Default values are $\alpha = .995$ and $\beta =
      .99$}. We use these thresholds to qualify whether the
      similarity between two pyroprints is high enough to warrant grouping
      their isolates together, or low enough to guarantee that their isolates
      should not be grouped together. Since these thresholds may vary depending
      on the ITS region and the selected FIB, each ITS region
      $r_i$ is associated with its own pair of thresholds, $\alpha_i$ and $\beta_i$.
      More specifically, if the similarity between two pyroprints is greater
      than $\alpha$ ($sim(\bar{p_1}, \bar{p_2}) > \alpha$) then the two
      pyroprints are considered the same, or identical. Conversely, if
      $sim(\bar{p_1}, \bar{p_2} \le \beta$, then the two pyroprints are
      considered \textit{definitely} different. The range between $\alpha$ and
      $\beta$ is colloquially called the \textit{squishy region} and
      similarities that fall in that range are inconclusively similar or
      dissimilar because we do not know whether it is experimental error or
      actual differences that account for the low similarity. The way this is
      handled is described in Chapters \ref{chap:algorithm} and
      \ref{chap:implementation} as appropriate.

   \section{Clustering}\label{sec:clustering}
      There are many datasets where relationships may exist that are not easily
      observable to the human eye. The analysis of data based on similarity
      functions determined by various relationships is known as cluster
      analysis, clustering, or unsupervised learning. phrased another way,
      clustering is the organization of a collection of patterns, or data
      points, based on similarity~\cite{Jain:DataClustering}. The groups or
      partitions that are formed during clustering are referred to as
      \textit{clusters}. Clustering is used for the analysis of
      many different types of data and although it can be done many ways, there
      are two primary families of clustering algorithms--hierarchical and
      partitional.

      Partitional clustering algorithms iteratively assign and re-assign data
      points to cluster centroids or seeds. A cluster centroid is a single data
      point selected to represent the ``center'' of a cluster. Typically,
      centroids are initially selected from random or maximally distant data
      points and gradually drift at the end of each iteration. How data points
      are assigned to cluster centroids on each iteration is how many
      partitional algorithms differ. K-means clustering approaches this by
      assigning data points to the cluster of the closest centroid on each
      iteration. For k-means, clusters are mutually exclusive and when a data
      point is assigned to a cluster, it is un-assigned to its previous
      cluster. Generally, due to the use of centroids, partitional clustering
      algorithms are most effective when there is a general idea of how many
      partitions exist in the input dataset. For the required analysis of
      pyroprinting, for which there is no clear idea of how many partitions
      exist, partitional clustering algorithms are not very useful, and
      therefore not explored further, except in Chapter \ref{chap:related}.
      
      Hierarchical clustering algorithms iteratively group a pair of data
      points that are maximally similar or separate data points that are
      maximally dissimilar. Hierarchical clustering algorithms analyze data
      points in a pairwise manner. Additionally, hierarchical clustering
      algorithms do not re-assign data points to different clusters. Because
      data points are only grouped with maximally similar counterparts, the
      order of clustering for a given dataset is consistent and unchanging. As
      clusters are formed or separated, hierarchical clustering algorithms
      construct a dendogram bottom-up or top-down, respectively. A dendogram is
      a binary tree where an internal node represents a cluster composed of two
      data points or sub-clusters represented by child nodes. Dendograms always
      represent individual data points as leaves and clusters as internal
      nodes. Since hierarchical clustering algorithms do not require any
      knowledge of how many partitions may exist in the dataset, this is the
      type of clustering algorithm further explored for analysis of
      pyroprints.

      \subsection{Agglomerative Hierarchical Clustering}\label{sec:agglomerative}
         Agglomerative hierarchical clustering is the standard bottom-up
         hierarchical clustering approach. Agglomerative hierarchical
         clustering initially considers each data point as a cluster and joins
         the pair of clusters with the maximal similarity on every iteration.
         This is known as a bottom-up approach because the dendogram for the
         input dataset is built from the leaves--each data point--to the
         root--all data points clustered together. There are two primary
         components of the agglomerative hierarchical clustering algorithm: the
         similarity metrics used between data points and clusters, and the
         threshold for cutting the dendogram. 
         
         The two primary similarity functions that must be computed during
         agglomerative hierarchical clustering are: similarity between data
         points, and similarity between clusters. The similarity function
         between data points is determined independent of the clustering
         algorithm and is based on the data being analyzed. The similarity
         function between clusters consists of three standard approaches:
         \begin{itemize}
            \item Single-Link
            \item Complete-Link
            \item Average-Link
         \end{itemize}
         The single-link approach computes a similarity function for each pair
         of data points in the cross-product between two clusters as the
         similarity between the two most similar data points between them,
         whereas the complete-link approach computes the opposite--the
         similarity between the two least similar data points between two
         clusters. Finally, average-link computes the similarity between two
         clusters as the average of all pairwise similarities between each data
         point in each cluster.

         Once clustering is complete and a dendogram is constructed, the
         resulting set of clusters must still be determined. The dendogram is
         constructed only to provide an analysis of the relationships between
         the data points in the input dataset. Since the root of the dendogram
         always represents all data points of the dataset as a single cluster,
         there is a ``cut'' that must be made in the dendogram that determines
         at what point joining clusters is no longer meaningful. This point is
         called a threshold--a value, or level, that a similarity must exceed.
         Intuitively, the threshold can be described as the point at which two
         clusters are believed to contain highly related, or even identical,
         data points. This will have different interpretations depending on the
         data being analyzed and the objective of the analysis being carried
         out, and so is determined based on the input dataset. The dendogram
         cut is made across edges between internal nodes and child nodes.
         Dendogram nodes above the cut are ignored, and dendogram nodes
         directly below the cut are considered the final set of clusters. In
         this way it is plain that nodes lower in the dendogram have higher
         similarity between sub-clusters than nodes higher in the dendogram.

\chapter{Biological Tools for Analysis}\label{chap:other_contributions}

\chapter{OHClust!}\label{chap:algorithm}
      In general, the hierarchical tree is represented as a general n-ary tree as seen in Figure
      \ref{fig:general_structure}. The parts of the tree are described as follows:
      (node) Any item in the tree, depicted as circles in Figure
      \ref{fig:general_structure}, (root) First node in the tree, located at the top,
      (leaf) A bottom-most node in the tree, (edge) A connection between any two
      nodes, (parent) In a pair of nodes with a direct connection, the parent node is
      the top node, or is closer to the root, (child) In a pair of nodes with a
      direct connection, the child node is the bottom node, or is further from the
      root, (depth) The number of edges to get from the root to a particular node,
      (level) Set of all nodes with the same depth.
      %\begin{description}
      %   \item[\em{node}] Any item in the tree, depicted as circles in Figure
      %   \ref{fig:general_structure}.
      %   \item[\em{root}] First node in the tree, located at the top.
      %   \item[\em{leaf}] A bottom-most node in the tree.
      %   \item[\em{edge}] A connection between any two nodes.
      %   \item[\em{parent}] In a pair of nodes with a direct connection, the parent
      %   node is the top node, or is closer to the root.
      %   \item[\em{child}] In a pair of nodes with a direct connection, the child
      %   node is the bottom node, or is further from the root.
      %   \item[\em{depth}] The number of edges to get from the root to a particular 
      %   node.
      %   \item[\em{level}] Set of all nodes with the same depth.
      %\end{description}

      If two isolates differ substantially, then the two isolates
      belong to different strains. The thresholds used for clustering are based
      on the thresholds given for each ITS region.

      Once strain similarity/dissimilarity of data points are quantified,
      hierarchical clustering methods (e.g., CLUSTAL or Primer5~\cite{Primer5:methods})
      are commonly used to analyze the obtained data and determine
      how many strains were present among the obtained bacterial isolates.
      
      If $sim(\bar{X}, \bar{Y})$ is
      sufficiently close to $1$, we assume $X$ and $Y$ come from the same strain.
      However, if $sim(\bar{X}, \bar{Y})$ is sufficiently far from $1$, then $X$ and
      $Y$ \textit{definitely come from different strains}.

      As an aggregate of several, variable copies of a DNA region, pyroprints
      are patterns of DNA which capitalize on the differences between the
      replicates of the ITS regions and serve as the basis of comparison
      between \textit{E. coli} isolates. A ``match'' between \textit{E. coli}
      isolates is determined when the pyroprints for both ITS 1 and 2 are
      considered identical between two isolates.

      \textbf{Data Transformation}. Hierarchical clustering algorithms, including the
      algorithm we developed here, work by comparing, on each step, groups of data
      points (pyroprints, in our applications) and combining similar data points to
      each other. Our algorithm uses a \textit{thresholded version} of the Pearson
      correlation coefficient to compare individual pyroprints to each other (see
      Equation \ref{eq:threshold}. Anytime inter-cluster similarity measures are
      calculated, our algorithm applies a threshold function (Equation
      \ref{eq:threshold}) which returns $1$ if the two pyroprints are the same, and
      $0$ if the respective pyroprints are definitely dissimilar. These cases are
      represented by the relationships $sim(\bar{X}, \bar{Y}) \ge \alpha$ and
      $sim(\bar{X}, \bar{Y}) < \beta$, respectively. Similarities between $\alpha$ and
      $\beta$ are left unmodified. This transformation is the mechanism for ensuring
      strongly connected isolates at the core of each cluster. We represent already
      constructed clusters of pyroprints using a single pyroprint vector representing
      average values of the cluster known as the \textit{average-link} hierarchical
      clustering method. Average-link inter-cluster similarity measures are
      calculated using Equation \ref{eq:cluster_sim}. In this function, $\alpha_r \in
      A$, $\beta_r \in B$ $\forall r \in R$ represents the threshold pair for each
      ITS Region $r$. $C$ is the set of clusters such that $C_m \in C$ and $\exists i
      \in I$ such that $i \in C_m$.

      \textbf{Clustering Algorithm}. Previous data sets used for analysis contained a
      large number of isolate pairs whose pyroprints had similarity between $0.995$
      and $1$. In such situations, knowing that two pyroprints collected from the
      same sample have a high similarity is sufficient to put them into a single
      cluster \textit{immediately}, even though a pyroprint from another month may
      have a higher similarity with one of them. In consideration of this, the
      algorithm we have developed constructs clusters in a two step process: (1)
      cluster all isolates that are \textit{strongly connected}\footnote{A set of
      pyroprints $P$ is considered \textit{strongly connected} if each pyroprint $p
      \in P$ is sufficiently similar (i.e. \textit{connected}) to every other
      pyroprint in $P$} in an order determined by the user; (2) cluster the remaining
      isolates with the clusters formed in step 1 using traditional hierarchical
      clustering. By integrating the remaining isolates into clusters in the last
      step, we minimize the negative effect that \textit{weakly connected} isolates
      have on cluster construction.

      The structure depicted by Figure \ref{fig:ontology_structure} dictates
      clustering order. Conceptually, this process occurs in the following manner:
      isolates located in the leaves of the structure are clustered using traditional
      hierarchical clustering. The resulting clusters are then propagated up the tree
      to the parent nodes. Optionally, the user may specify whether to incrementally
      grow clusters or to integrate all clusters together at once. This specific
      implementation of the approach incrementally grows clusters. This
      incremental growth is representative of the approximate chronology in which
      isolates were collected. This process repeats%for each swabbing technique, and
      for each month in the study until clustering of all isolates in the structure
      is complete.

      \begin{figure}[H]
      \centering
      \subfloat[General]{
         \label{fig:general_structure}
         \includegraphics[width=0.5\textwidth,height=12em]{graphics/IsolateTree.eps}
      }
      \subfloat[Experimental]{
         \label{fig:cluster_structure}
         \includegraphics[width=0.5\textwidth,height=12em]{graphics/IsolateTree_CurrentStudy.eps}
      }
      \caption{Isolate Tree - Structural Organization: The general structure
               illustrates the parts of the organizational structure, whereas
               the experimental structure represents the month that the
               isolates were collected in the first level and the swabbing
               technique used to collect the isolate in the second level.}
      \label{fig:ontology_structure}
      \end{figure}


   \section{OHClust!}\label{sec:ohclust}
      \subsection{Algorithm}
         \begin{enumerate}
            \item create feature ontology
            \item parse data (multiple matrices or from database)
            \item organize data into feature ontology
            \item walk tree nodes and cluster where data is present (leaves)
            \item when all children of a node have been clustered, the data in the
                  node is clustered, and these clusters are returned up to the
                  parent
         \end{enumerate}

         \begin{algorithmic}
            \Function {clusterDataStructure}{$D$, $N$}
               \State $C \gets \emptyset$

               %If this node is null then return emptyset
               \If {$N = null$}
                  \Return $C$
               \EndIf

               %If this node has children then cluster them before this node
               \If {$|$children($N$)$| > 0$}
                  \For {$n_i \in$ children($N$)}
                     \State $C \gets C\ \cup$ clusterDataStructure($D$, $n_i$)

                            \If {isChronological($N$)}
                        \State $C \gets C\ \cup$ performHierarchical($D$, $C$)
                     \EndIf
                  \EndFor

                  \If {!isChronological($N$)}
                     \State $C \gets C\ \cup$ performHierarchical($D$, $C$)
                  \EndIf

               %If this is a leaf node then just cluster the data
               \Else
                  \State $C \gets$ performHierarchical($D$, $C$)
               \EndIf

               %Return clusters to parent
               \Return $C$
            \EndFunction

            \Function {performHierarchical}{$D$, $C$}
               \State $C' \gets C$
               \State ($c_a$, $c_b$) $\gets \emptyset$
               \State $S[j,k] \gets$ recomputeDistances($C'$)
               \For {$c_j$, $c_k$ $\in C'$}
                  $S[j, k] \gets$ sim($c_j$, $c_k$)
               \EndFor

               \While {$|C'| > 1$ and sim($c_a$, $c_b$) $\ge \alpha$}
                  \State ($c_a$, $c_b$) $\gets$ \textit{arg}min($S[j,k]$)
                  \If {sim($c_a$, $c_b$) $\ge \alpha$}
                     \State $C' \gets C'\ \cup$ combineClusters($c_a$, $c_b$)
                     \State $S[j,k] \gets$ recomputeDistances($C'$)
                  \EndIf
               \EndWhile

               \For {$c_i \in C$}
                  \If {$|c_i = 1$}
                     \State $W \gets W\ \cup c_i$
                  \EndIf
               \EndFor
            \EndFunction

            \Function {recomputeDistances}{$S$, $C$}
               \For {$c_j$, $c_k \in C$}
                  \If {sim($c_j$, $c_k$) $\ge \alpha$}
                     \State sim($c_j$, $c_k$) $\gets 1$
                  \ElsIf {sim($c_j$, $c_k$) $< \beta$}
                     \State sim($c_j$, $c_k$) $\gets 0$
                  \EndIf
               \EndFor

               \Return $S$
            \EndFunction
         \end{algorithmic}

\chapter{Related Work}\label{chap:related}

   \section{Incremental Clustering}\label{sec:incr_cluster}
   Since OHClust! is claimed to be applicable to data types other than
   biological, this section discusses incremental clustering algorithms
   developed for generic datasets. These algorithms were developed to
   incrementally cluster data streams or datasets too large to fit in main
   memory. While this definition of incremental clustering is still applicable
   and relevant to our notion of incremental clustering, there are some
   assumptions that may cause divergence between the optimal clustering of a
   growing dataset and proposed clustering of these algorithms. More
   specifically, clustering a dataset in increments, still has the assumptions
   of a single, whole dataset. In contrast, clustering a dataset that is
   updated in increments has the assumptions that the ideal number of clusters
   may change between updates. The subtle difference may make a difference in
   how quickly produced clusters diverge from optimal clusters, and thus affect
   how frequently the accumulated dataset must be re-clustered.

   %BIRCH description%
   BIRCH, Balanced Iterative Reducing and Clustering using Hierarchies, is a
   clustering method developed by Zhang, et al. to address the problem of
   clustering large datasets and minimizing I/O costs~\cite{Zhang:BIRCH}. BIRCH
   incrementally clusters numerical data by doing a scan of the target dataset
   until memory constraints are reached. Each chunk of data is added to an
   N-ary tree, called a cluster feature (CF) tree, which represents clusters as
   internal nodes and maintains only aggregate information for each cluster.
   The aggregate information, and clustering requirements, are computed based
   on basic algebraic functions: Each data point is represented as a triple of
   the form $D = (N, LS, SS)$ where a data point $D$ is assumed to be a vector,
   $N$ is the number of dimensions, $LS$ is the linear sum of the dimensions of
   $D$, and $SS$ is the squared sum of the dimensions of $D$. In this way,
   distances (not similarities) are additive and easily computed in a single
   scan of the data. This approach uses the CF Tree as a fast, guiding method
   for directing a data point to the appropriate cluster. However, determining
   an appropriate, meaningful CF representation is difficult at best.

   %Fast and Stable Incremental Clustering description%
   Young, et al.'s work on incremental clustering takes a 
   partitional approach to clustering~\cite{Young:Incremental}. The described
   work utilizes a competitive learning algorithm that adjusts itself based on
   a calculated pseudo-entropy of the clusters such that a
   tradeoff is made between updating centroids aggressively earlier in the
   clustering process (earlier iterations and/or updates)
   and updating centroids conservatively later to ensure cluster stability.
   Additionally, there is a credit-based algorithm for preventing centroid
   starvation which is computed based on a fixed value or the previously
   calculated pseudo-entropy such that centroids selected for updates lose
   credit and centroids that are starving accumulate credit over time. Although
   Young, et al. mention the importance of being mindful of the possibility
   that clusters may move, disappear, or reappear, the described work only
   discusses moving centroids and assumes a fixed number of centroids. Since
   the number of clusters that may be formed from clustering pyroprints is
   completely unknown, and will grow unpredictably as more data is gathered
   from different regions, a partitional scheme with a fixed number of
   centroids is not very applicable.

   \section{Incremental Clustering for 16S rRNA Sequences}\label{sec:seq_incr_cluster}
   This section discusses incremental clustering algorithms that were
   developed specifically for clustering DNA sequence reads. These are
   especially relevant to the work described in this paper as OHClust! was
   developed with pyroprints in mind. Incremental clustering algorithms
   described in this section are all derived from a greedy incremental sequence
   clustering algorithm, hereafter referred to as \textit{SeqClustering} for
   brevity, developed in 1998 by Holm and Sander~\cite{Holm:Greedy}. The
   motivation for this incremental clustering algorithm was to effectively
   handle continuously growing biological datasets, specifically protein
   sequences, and to efficiently determine and convey clusters of similar
   protein sequences. Due to the high volume of redundant protein sequence
   reads, Holm and Sander decided that each cluster should be represented by a
   single data point, or sequence read. By using a representative data point
   for each cluster, it was much more efficient to compute cluster membership
   for each unclustered data point and to convey meaningful, non-redundant
   information for understanding each cluster. Variants of this clustering
   algorithm refer to the representative sequence read of a cluster as a
   \textit{seed}. This algorithm, as well as its variants such as
   CD-HIT~\cite{Li:CD_Hit}, UCLUST~\cite{Edgar:UCLUST}, and
   DySC~\cite{Zheng:DySC}, cluster DNA sequences by using a single sequence as
   a cluster representative to which unclustered DNA sequences are aligned
   against. That is, DNA sequences are data points, and each cluster is
   represented by a single data point, against which other data points are
   compared. The comparison metric used is typically a sequence alignment,
   which gives a similarity score based on the edit distance between the
   compared sequences.

   One variant of Holm and Sander's SeqClustering algorithm is cd-hit and each
   of its variants: cd-hit-2d, cd-hit-est, cd-hit-est-2d~\cite{Li:CD_Hit,
   Li:Redundancy}. Cd-hit has SeqClustering as its core algorithm, but
   optimizes performance by using short word filtering instead of sequence
   alignments as its comparison metric. Short word filtering, in short,
   verifies that the compared sequences share a minimum number of identical
   short substrings, referred to as `words', such as dipeptides, tripeptides,
   etc. For certain length words, it's possible to have indices of the seed for
   a cluster so that it is very fast to compare a sequence to the seed via
   short word filtering.

   %DySC Greedy Clustering Description%
   Another, very relevant, variant of the SeqClustering algorithm, called
   dynamic seed clustering (DySC), was developed by Zheng, et
   al.~\cite{Zheng:DySC}. DySC is a clustering algorithm developed for reads of
   the 16S rRNA marker gene commonly used in microbial studies. DySC
   differentiates itself from other SeqClustering algorithms by using both
   fixed seeds and dynamic seeds. Seeds in SeqClustering correspond to a fixed
   seed in DySC, whereas SeqClustering has no equivalent to dynamic seeds in
   DySC. Using dynamic seeds, DySC constructs \textit{pending clusters},
   clusters containing reads that are not within a similarity threshold to the
   fixed seeds. Pending clusters are transient and will either join a fixed
   seed cluster or become a fixed seed cluster after reaching a specified size.
   Zheng, et al. claim that DySC is able to improve cluster quality while
   maintaining comparable runtime compared to UCLUST and
   CD-HIT~\cite{Zheng:DySC}.

   %Incremental Clustering Description%
   Yooseph, et al. have done work on the incremental clustering of microbial
   metagenomic sequence data~\cite{Yooseph:Incremental}. Incremental clustering
   is a three stage clustering process based on CD-HIT. \textbf{First Stage}.
   patterns are combined with clusters in three steps, each with 90\%, 75\%,
   and 60\% identity thresholds, respectively. Incremental clustering takes
   this approach of decreasing identity thresholds for efficiency and quality.
   For efficiency, cd-hit-2d runs faster at high thresholds (90\%) than at low
   thresholds (60\%). For quality, the parallel implementation of cd-hit-2d
   being used by Yooseph, et al. at the time (2008) could only assign patterns
   to the first cluster whose similarity met the threshold. \textbf{Second
   Stage}. Patterns from the data set not incorporated into clusters in stage
   one are clustered together using cd-hit at 90\%, 75\%, and 60\% identity.
   The clusters formed here are referred to as \textit{core clusters} by
   Yooseph, et al. \textbf{Third Stage}. Two similarity measures are used to
   join \textit{big core clusters} (cluster with cardinality $\ge$ 20) with
   each other and small core clusters or singleton clusters (cluster with
   cardinality $=$ 1) with final clusters, respectively FFAS and PSI-BLAST.
   
   %Relate algorithms to work%
   Each variant of the SeqClustering algorithm is highly relevant to OHClust!
   in their ability to cluster data incrementally and their relevance to
   biological data. Using a seed, or cluster representative, that is $\le 90\%$
   similar to all other seeds allows new clusters to easily form, which is
   important to consider for a continuously growing dataset. Interestingly,
   Yooseph, et al.'s work on incremental clustering and its three phase
   approach seems to be the most similar to OHClust!. Where Yooseph, et al.
   takes a three phase approach, OHClust! seeks to improve clustering
   performance and quality in a two phase approach. The first phase creates
   core clusters using the $\alpha$ threshold, and the second phase
   incorporates pyroprints into a boundary or fuzzy cluster using the $\beta$
   threshold. However, incremental clustering partitions its data in a three
   tier scheme by thresholds (90\%, 75\%, and 60\%) instead of the hierarchical
   partitioning employed by OHClust!.
   
   Seemingly, these algorithms are more accommodating of updates than
   incremental clustering in Section \ref{sec:incr_cluster} and can handle
   situations where constructed clusters may significantly differ from initial
   predictions of observable patterns. In this way, these algorithms may be
   more amenable to incremental clustering across updates than OHClust!.
   However, there is a major aspect in which SeqClustering algorithms differ
   from OHClust!: the use of a core cluster in OHClust! instead of a
   representative data point. Instead of a representative data point, the use
   of core clusters potentially reduces clustering errors of pyroprints,
   especially across updates. This is especially important since pyroprints are
   not exactly DNA sequences and are vulnerable to machine and human error.
   Additionally, cluster similarity is computed using each point in the cluster
   (in the case of average-link or ward's method) instead of a single
   comparison. Due to this being computationally more expensive, the advantages
   and disadvantages of not using cluster representatives are explored in
   Chapter \ref{chap:evaluation}.

   %Temporal clustering, as described by Kamath and
   %Caverlee~\cite{Kamath:Transient}, is a variation of clustering that models
   %data as a communication network. Nodes represent members in the
   %communication network while edge weights represent communication between
   %said nodes. The edge weights in the network are based on when the messages
   %are exchanged. Kamath and Caverlee utilize 3 different edge weight decay
   %functions to reflect temporal locality. Their naive function decrements edge
   %weights by 1 between two nodes where a message was not exchanged by the two
   %nodes during a specific time interval. Otherwise the edge weight is
   %incremented by 1. Another suggested decay function, \textit{fixed window},
   %only utilizes edge weights where communication occured during a time window
   %$\beta$. Edge weights are ignored if communication between two nodes occurs
   %outside of this time window. The final edge weight decay function proposed,
   %\textit{exponential decay}, is the final function proposed in
   %\cite{Kamath:Transient} and was the chosen function used in Kamath and
   %Caverlee's temporal clustering. This uses a parameter $\xi$ that is used to
   %identify crowds gathering at relative rates. A higher $\xi$ is used for
   %identifying crowds which form quickly and a lower $\xi$ identifies crowds
   %forming slowly. More formally:
   %\textit{Messages not exchanged:}
      %\begin{description}
         %\item w$_{t}$(u, v) = w$_{t - 1}$(u, v) - log(T$_{now}$ - $\tau$(u, v))
               %$\times$  $\xi$ 
      %\end{description}

   %\textit{Messages exchanged:}
      %\begin{description}
         %\item w$_{t}$(u, v) = w$_{t - 1}$(u, v) + 1 - log(T$_{now}$ - $\tau$(u, v))
               %$\times$  $\xi$ 
      %\end{description}
   %$\tau$(u, v) is the time of the last communication between nodes u and v. This
   %way the longer the gap in communication between two nodes, the lower the edge
   %weight connecting the two nodes.

   %Qamra, Tseng, and Chang~\cite{Qamra:BlogMining} suggest a \textit{modified
   %time-sensitive dirichlet process model} based on work by Jhu, et
   %al.~\cite{Jhu:Dirichlet}. This clustering algorithm is similar to the
   %Chinese Restaurant Process but also incorporates
   %time~\cite{Qamra:BlogMining}. The probability of joining a group (cluster) depends on the
   %group members and their respective ages. There is also some probability,
   %determined by a \textit{concentration parameter}, that a new cluster will be
   %formed. As group members age, their respective influence in the group decays.
   %The time-sensitive dirichlet discussed by Jhu, et al.~\cite{Jhu:Dirichlet}
   %was modified by Qamra, et al. to calculate a uniform probability of being
   %assigned a new cluster. The decay of a blog story's influence in a cluster
   %is controlled by a kernel function so that new entries are less likely to
   %join older stories.

   %\textit{PoClustering} (partially ordered clustering) clusters data into
   %\textit{PoSets} (partially ordered sets) by finding all clique clusters for
   %all possible diameters W(D) where D is the maximal dissimilarity in a
   %dissimilarity matrix~\cite{Liu:CPD}. PoClustering is a generalization of both
   %hierarchical and pyramidal clustering that allows overlaps between clusters such that a
   %PoCluster P is defined as P = \{cliqueset $_{\delta}$ (d) $\mid$ $\forall$ d $\in$
   %W(D)\}~\cite{Liu:PoClustering}. This ensures that PoClusters contain every
   %possible cluster with largest dissimilarity d. Additionally, hierarchical
   %and pyramidal clustering produce clusters that are subsets (special cases) of
   %PoClusters. PoClusters can be represented as a directed acyclic graph, with
   %each node representing a clique cluster and its diameter, and each edge
   %representing subset relationships between nodes. PoClusters are able to
   %successfully preserve the majority of relationships present in the data
   %whereas hierarchical clustering is unable to do so.

   %Temporal clustering and time-sensitive dirichlet modify similarity measures
   %in context of the temporal locality of particular events of interest. This
   %is slightly different from the method we propose in this paper, as we do not
   %modify similarity measures of \textit{E. coli} isolates based on
   %chronological distance. Instead, we simply enforce a particular ordering on
   %cluster candidates based on a given, defined ontological structure. 
   %Similarly, PoClustering enforces a particular ordering on the clustering
   %process without modifying similarity measures. Although PoClustering is not
   %time-sensitive, it is important related work for our method regarding
   %connectivity constraints between \textit{E. coli} isolates.

   %TODO
\chapter{Implementation}\label{chap:implementation}
   %The implementation described in this chapter was done in Java, and compliant
   %with Java 1.6 since the majority of users have Java 1.6 installed, not Java
   %1.7. Additionally, the code is accessible from the SPAM repository on my
   %GitHub account\footnote{http://www.github.com/drin/spam}. This chapter also
   %has a dual purpose of describing how the algorithm has been implemented, and
   %providing a reference of documention for anyone desiring to use or extend
   %the code provided.

   \section{SPAM - Suite of Pyroprint Analysis Methods}\label{sec:spam}
      This section discusses the implementation of the framework
      \textit{SPAM}--Suite of Pyroprint Analysis Methods. This framework is
      written such that many components can be written consistently and
      interchangeably, including clustering methods, comparison metrics, and
      data types. In Section \ref{sec:design} the overall design of
      the framework is described, while Sections \ref{sec:data_types} and \ref{sec:metrics}
      discuss how data types can be written, and how comparison metrics are
      called and can be implemented, respectively. Finally, Section
      \ref{sec:analysis} details how analysis methods--particularly
      hierarchical clustering methods--can be extended and customized.

      \subsection{Design}\label{sec:design}
         The framework was designed to be modular. Particularly, it was desired
         that it would be easy to swap comparison metrics and clustering
         methods. In order to do this, an abstract approach for defining and
         using various data types had to be designed.
         
         The basic design is that custom data types extend a
         \textit{Clusterable} abstract class interface.


         However, to ensure a flexible and
         maintainable system, isolates are represented as a complex data point
         that is associated with \textit{ITS Regions} and \textit{Pyroprints}.

      \subsection{Data Types}\label{sec:data_types}

      \subsection{Comparison Metrics}\label{sec:metrics}

      \subsection{Analysis Methods}\label{sec:analysis}

\chapter{Evaluation}\label{chap:evaluation}
   For evaluation of OHClust!, resulting clusters are compared against the
   clusters of \textit{Agglomerative Hierarchical
   Clustering}--\textit{HClustering} for short. Given the wide use of
   HClustering by biologists on biological datasets, HClustering is used as
   the baseline to compare OHClust! against. Preliminary results on the
   comparison of accuracy was done in a previous case
   study~\cite{Montana:ChronoCluster}, but, in this section, the
   clusters produced by the two algorithms are compared more thoroughly, on larger, more
   complex datasets, and both algorithms are analyzed for their performance. In many
   cases, OHClust! is expected to produce clusters $C$ that are a
   \textit{refinement} of clusters from HClustering, $C'$. That is, each
   cluster $C_i \in C$ is contained in a cluster of $C'_k \in C'$,
   formally\footnote{This definition of refined clusters comes from
   Wagner~\cite{Wagner:Overview}}:
   $$\forall C_i \in C, \exists C'_k \in C' : C_i \subseteq C'_k$$.
   Below, both the hypothesis and evaluation plan are described before discussing
   results in Section \ref{sec:results} and relevant conclusions in Section
   \ref{sec:conclusion}.

   \section{Hypothesis}\label{sec:eval_goals}
      For the comparison between OHClust! and HClustering, the resulting
      clusters of both algorithms are expected to be largely similar. This is
      formalized by the null hypothesis $H_0: VI(C, C')$ is very close to $2/n$
      (Not yet actually determined). $VI(C, C')$ is the variation of
      information~\cite{Halkidi:Validation} between the set of OHClust!
      clusters $C$ and the set of HClustering clusters $C'$ defined as:
      $$VI(C, C') = H(C) + H(C') - 2I(C, C')$$
      where:
      $$P(i) = \frac{C_i}{n}$$
      $$P(i,j) = \frac{| C_i \cap C_j'}{n}$$
      $$H(C) = -\sum_{i=1}^{k}P(i)log_2P(i)$$
      and
      $$I(C, C') = \sum_{i=1}^{k}\sum_{j=1}^{t}P(i,j)log_2\frac{P(i,j)}{P(i)P(j)}$$
      In the above equations, $n$ is the number of clusters, $P(i)$ is the
      probability that an element $x$, at random, is in the cluster $C_i \in C$
      and $P(i, j)$ is the probability that an element $x$ is in the clusters
      $C_i \in C$ and $C'_j \in C'$, where $|C_i \cap C'_j|$ is the size of the
      intersection between clusters $C_i$ and $C'_j$. shannon's entropy,
      $H(C)$, can then be calculated for $C$ and $C'$. Relating the entropy of
      both sets of clusters, $H(C)$ and $H(C')$, we then calculate the mutual
      information between $C$ and $C'$, $I(C, C')$.

      Additionally, it is expected that while the performance of OHClust! and
      HClustering should be relatively similar for a given raw dataset,
      OHClust! will perform much faster than HClustering when given additional
      data, that still fits the ontology specified, to be clustered. This
      expectation is based on the ability of OHClust! to analyze and
      incorporate clusters in newly collected data with clusters previously
      formed, whereas HClustering may incorrectly join new data to previously
      constructed clusters, and thus must completely re-analyze the entire
      dataset when new data is introduced.

   \section{Evaluation Plan}\label{sec:eval_plan}
      The evaluation plan detailed in this section focuses on the question
      ``how does the performance of OHClust! compare to HClustering?'' while at
      the same time ensuring that \textit{The clusters formed by OHClust! and
      HClustering do not diverge significantly.} The divergence between
      HClustering and OHClust! is important to consider since there is no clear idea
      of what resulting clusters \textbf{should} look like, so by showing that
      the clusters formed by each algorithm are similar enough to each other,
      it can be seen that OHClust! is no less accurate than HClustering, and
      thus, accuracy is not an issue.

      My evaluation plan is motivated by the following evaluation goals:
      \begin{itemize}
         \item Show that the clusters formed by OHClust! are insignificantly
               dissimilar from HClustering.
         \item Determine how many data points may be added before the
               dissimilarity between OHClust! and HClustering clusters
               significantly diverge.
         \item Assess the performance benefits of using OHClust! in a
               microbial source tracking system in place of HClustering. More
               specifically:
               \begin{itemize}
                  \item On average, how much time is saved using OHClust!?
                  \item In the worst case, where OHClust! must re-analyze the
                        entire dataset, what is the performance difference
                        between OHClust! and Hierarchical Agglomerative?
               \end{itemize}
      \end{itemize}

      The test harness for SPAM consists of three parameters: a configuration,
      a clusterer, and a data set. The configuration is a java class that
      contains all of the parameters needed by any clusterer. These
      parameters are all of the variables that we need to alter during our
      evaluation tests in order to fully characterize the analysis methods and
      their applicability:
      \begin{itemize}
         \item Similarity thresholds(i.e. $\alpha$ and $\beta$):
               \begin{itemize}
                  \item ITS Region
                  \item Cluster
               \end{itemize}
         \item Similarity transformations in relation to isolate similarity
               thresholds
         \item Pyroprint dispensation lengths (per ITS region)
         \item Inter-cluster and inter-ITS region distance metrics:
               \begin{itemize}
                  \item Single-link
                  \item Complete-link
                  \item Average-link
               \end{itemize}
      \end{itemize}
      Some properties of the two algorithms that should be observed:
      \begin{itemize}
         \item Compactness--members within a cluster should be as similar, or
               close, as possible.
         \item Separation--clusters should be as separated, or dissimilar, as
               possible.
      \end{itemize}

      (This is written as if the tests are already done) While there are many
      parameters that can be tuned to characterize the behaviors of OHClust!,
      It is unnecessary to compare the results from all configurations of
      OHClust! with all configurations of HClustering. The tests that have been
      ran include several configurations of OHClust! compared against each other, and the
      comparison of one configuration of HClustering to the same configuration
      of OHClust!. The different configurations of OHClust! that were tested
      can be seen in Figure \ref{tab:ohclust_configs}.

      \begin{table}[t]
      \centering
      {\small
      \begin{tabular} {| c | c | c | c | c | c | c | c | c |}
      \hline
         \textbf{T} & \textbf{Num Itrs} & \textbf{Itr Incr} & \multicolumn{2}{| c |}{\bf Metrics} \\
      \hline
                    &                   &                   & inter-cluster & inter-ITS     \\
      \hline                           
         Yes        & 50                & 50                & average-link  & average-link  \\
      \hline                                               
         No         & 50                & 50                & average-link  & average-link  \\
      \hline                                               
         Yes        & 100               & 25                & average-link  & average-link  \\
      \hline                                               
         No         & 100               & 25                & average-link  & average-link  \\
      \hline                                               
         Yes        & 100               & 50                & average-link  & average-link  \\
      \hline                                               
         No         & 100               & 50                & average-link  & average-link  \\
      \hline                                               
         No         & 100               & 50                & average-link  & average-link  \\
      \hline                                               
         No         & 100               & 50                & single-link   & average-link  \\
      \hline                                               
         No         & 100               & 50                & complete-link & average-link  \\
      \hline                                               
         No         & 50                & 50                & average-link  & median-link   \\
      \hline                                               
         No         & 50                & 50                & average-link  & complete-link \\
      \hline
      \end{tabular}
      }
      \caption{\textbf{OHClust! Test Configurations}. \textbf{Num Itrs}
               represents the number of iterations in which additional data is
               added to be clustered. \textbf{Itr Incr} represents the amount
               of data points added on each iteration. \textbf{T} represents
               whether similarities above $\alpha$ and below $\beta$ are
               transformed to $1$ and $0$ respectively. The listed metrics are
               for inter-cluster distance and inter-ITS region distance. These
               ones are listed in particular as they have the most influence on
               whether two clusters or data points will have a high similarity.}
      \label{tab:ohclust_configs}
      \end{table}


   \section{Results}\label{sec:results}

   \section{Conclusion}\label{sec:conclusion}

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
   %%%%%%%%%%%%%% ------------- End main chapters ---------------------- %%%%%%%%%
   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
   %%%%%%%%%%%%%% ------------- Begin appendices ----------------------- %%%%%%%%%
   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\appendix
%\chapter{Definitions}\label{app:definitions}

\clearpage
\bibliography{bibliography}
\bibliographystyle{plain}
%\addcontentsline{toc}{chapter}{Bibliography}

\end{document}

%\begin{tabular} {| c | c | c | c | c | c | c | c | c |}
%\hline
   %{\bf T}          & \multicolumn{2}{| c |}{\bf Length}     & \multicolumn{4}{| c |}{\bf R}                                                     & \multicolumn{2}{| c |}{\bf Metrics} \\
%\hline
                        %& 16S--23S    & 23S--5S              & $\alpha$ (16S)      & $\beta$ (16S)      & $\alpha$ (23S)     & $\beta$ (23S)     & inter-cluster & inter-ITS     \\
%\hline                                                                                                                                                                          
   %Yes                  & 93          & 93                   & $99.5$              & $99.0$             & $99.5$             & $99.0$            & average-link  & average-link  \\
%\hline                                                                                                                                                                          
   %No                   & 93          & 93                   & $99.5$              & $99.0$             & $99.5$             & $99.0$            & average-link  & average-link  \\
%\hline                                                                                                                                                                          
   %No                   & 92          & 92                   & $99.5$              & $99.0$             & $99.5$             & $99.0$            & average-link  & average-link  \\
%\hline                                                                                                                                                                          
   %No                   & 91          & 91                   & $99.5$              & $99.0$             & $99.5$             & $99.0$            & average-link  & average-link  \\
%\hline                                                                                                                                                                          
   %No                   & 90          & 90                   & $99.5$              & $99.0$             & $99.5$             & $99.0$            & average-link  & average-link  \\
%\hline                                                                                                                                                                          
   %No                   & 93          & 93                   & $99.5$              & $99.0$             & $99.5$             & $99.0$            & single-link   & average-link  \\
%\hline                                                                                                                                                                          
   %No                   & 93          & 93                   & $99.5$              & $99.0$             & $99.5$             & $99.0$            & complete-link & average-link  \\
%\hline                                                                                                                                                                          
   %No                   & 93          & 93                   & $99.5$              & $99.0$             & $99.5$             & $99.0$            & average-link  & median-link   \\
%\hline
   %No                   & 93          & 93                   & $99.5$              & $99.0$             & $99.5$             & $99.0$            & average-link  & complete-link \\
%\hline
%\end{tabular}
%\caption{\textbf{OHClust! Test Configurations}. Each row is a different
         %configuration of an OHClust! test run. The \textbf{T} column
         %represents whether similarities above $\alpha$ and below $\beta$
         %in an ITS region are transformed to $1$ and $0$ respectively.
         %The \textbf{Length} column represents the number of
         %dispensations used for each ITS region. The \textbf{R} column
         %represents the set of $\alpha$ and $\beta$ thresholds for each
         %ITS region starting with 16S or 23S.}
